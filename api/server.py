import sys
import os
import cv2
import numpy as np
import uvicorn
import pynvml
from contextlib import asynccontextmanager
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from typing import List

# --- Path Setup ---
# Proje ana dizinini path'e ekliyoruz ki 'inference' ve 'monitoring' modÃ¼llerini bulabilsin
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from api.schemas import DetectionResponse, HealthResponse, MetricsResponse, BoundingBox
# Ã–nceki adÄ±mlarda yazdÄ±ÄŸÄ±mÄ±z gÃ¼Ã§lÃ¼ Detector sÄ±nÄ±fÄ±nÄ± kullanÄ±yoruz
from inference.inference import Detector
# Ä°zleme iÃ§in Dashboard sÄ±nÄ±fÄ±nÄ± kullanÄ±yoruz
from monitoring.dashboard import Dashboard

# --- Configuration ---
MODEL_PATH = "../models/yolov8l_int8.engine"  # Veya .engine
CONFIDENCE_THRESHOLD = 0.5

# --- Global State ---
# Detector ve Dashboard'u global olarak tanÄ±mlÄ±yoruz
detector: Detector = None
# Dashboard'u burada baÅŸlatÄ±yoruz, her istekte yeniden baÅŸlatmak (file'daki hata) performansÄ± Ã¶ldÃ¼rÃ¼r
dashboard = Dashboard(window_size=100)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Uygulama baÅŸlarken modeli yÃ¼kler ve GPU izlemeyi kontrol eder.
    """
    global detector
    print("ğŸš€ Server starting up...")

    # 1. Load Model (Detector SÄ±nÄ±fÄ± Ã¼zerinden)
    if os.path.exists(MODEL_PATH):
        try:
            # Backend olarak 'tensorrt' seÃ§iyoruz. inference.py bu iÅŸi halledecek.
            print(f"Loading TensorRT Engine: {MODEL_PATH}")
            detector = Detector(backend="tensorrt", model_path=MODEL_PATH, conf_thres=CONFIDENCE_THRESHOLD)
            # IsÄ±nma turu (Warmup) Detector __init__ iÃ§inde otomatik yapÄ±lÄ±yor.
        except Exception as e:
            print(f"âŒ Critical Error loading model: {e}")
            detector = None
    else:
        print(f"âš ï¸ Warning: Model not found at {MODEL_PATH}. API will return errors.")

    # 2. GPU Monitoring KontrolÃ¼
    if dashboard.gpu_available:
        print("âœ… GPU Monitoring Initialized (NVML)")
    else:
        print("âš ï¸ GPU Monitoring Disabled (NVML Init Failed)")

    yield

    # Shutdown logic
    print("ğŸ›‘ Server shutting down...")
    try:
        if dashboard.gpu_handle:
            pynvml.nvmlShutdown()
    except:
        pass


# --- API Application ---
app = FastAPI(
    title="Real-Time Detection API (TensorRT)",
    version="1.0.0",
    lifespan=lifespan
)


def decode_image(file_bytes: bytes) -> np.ndarray:
    """Bytes verisini OpenCV formatÄ±na Ã§evirir."""
    nparr = np.frombuffer(file_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if img is None:
        raise HTTPException(status_code=400, detail="Invalid image format")
    return img


# --- Endpoints ---

@app.get("/health", response_model=HealthResponse)
def health_check():
    """API ve GPU durumunu kontrol eder."""
    return {
        "status": "healthy" if detector else "degraded",
        "gpu_available": dashboard.gpu_available
    }


@app.post("/detect", response_model=DetectionResponse)
async def detect_objects(
        background_tasks: BackgroundTasks,
        file: UploadFile = File(...)
):
    """
    Ana tespit endpoint'i.
    1. Resmi decode eder.
    2. Detector ile tahmin yapar (SÃ¼reyi Ã¶lÃ§erek).
    3. SonuÃ§larÄ± loglar.
    """
    if not detector:
        raise HTTPException(status_code=503, detail="Model not initialized")

    # 1. Read & Decode
    try:
        contents = await file.read()
        image = decode_image(contents)
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

    # 2. Inference & Monitoring
    # Timer'Ä± baÅŸlat
    t0 = dashboard.start_recording()

    try:
        # Detector sÄ±nÄ±fÄ± hem sonuÃ§larÄ± hem de sÃ¼re istatistiklerini (stats) dÃ¶ner
        # inference.py iÃ§indeki __call__ metodunu kullanÄ±yoruz
        detections_raw, stats = detector(image)
    except Exception as e:
        print(f"Inference Error: {e}")
        raise HTTPException(status_code=500, detail="Inference failed")

    # Timer'Ä± durdur ve sÃ¼reyi kaydet
    latency_ms = dashboard.stop_recording(t0)

    # 3. Background Logging
    # YanÄ±tÄ± geciktirmemek iÃ§in loglamayÄ± arka plana atÄ±yoruz
    background_tasks.add_task(dashboard.capture_snapshot)

    # 4. Format Response
    # Detector sÄ±nÄ±fÄ± zaten {box, score, class_id} formatÄ±nda dÃ¶nÃ¼yor, ÅŸemaya uyarlÄ±yoruz
    formatted_detections = []

    # inference.py'den gelen formatÄ± API ÅŸemasÄ±na Ã§eviriyoruz
    # Gelen format: [{'box': [x1, y1, x2, y2], 'score': 0.95, 'class_id': 0}, ...]
    for det in detections_raw:
        box = det['box']
        formatted_detections.append(BoundingBox(
            x_min=int(box[0]),
            y_min=int(box[1]),
            x_max=int(box[2]),
            y_max=int(box[3]),
            confidence=det['score'],
            class_id=det['class_id'],
            label=f"class_{det['class_id']}"  # EÄŸer class names listen varsa buradan maple
        ))

    # FPS'i dashboard Ã¼zerinden anlÄ±k hesaplÄ±yoruz
    current_fps = dashboard.meter.get_fps()

    return {
        "detections": formatted_detections,
        "inference_time_ms": round(latency_ms, 2),
        "fps": round(current_fps, 1),
        "model_name": "yolov8_trt"
    }


@app.get("/metrics", response_model=MetricsResponse)
def get_metrics():
    """
    Dashboard Ã¼zerinden canlÄ± performans verilerini Ã§eker.
    """
    # Dashboard sÄ±nÄ±fÄ±ndaki FPSMeter'dan istatistikleri alÄ±yoruz
    lat_stats = dashboard.meter.get_latency_statistics()
    gpu_stats = dashboard._get_gpu_stats()
    current_fps = dashboard.meter.get_fps()

    return {
        "avg_latency_ms": lat_stats["avg"],
        "current_fps": round(current_fps, 1),
        "gpu_usage_percent": gpu_stats["util_pct"],
        "gpu_memory_used_mb": gpu_stats["mem_used_mb"]
    }


if __name__ == "__main__":
    uvicorn.run("api.server:app", host="0.0.0.0", port=8000, reload=False)